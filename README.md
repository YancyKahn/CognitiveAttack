# CognitiveAttack
A novel red-teaming framework that exploits cognitive biases to bypass safety mechanisms in Large Language Models (LLMs).


## Overview
CognitiveAttack systematically leverages both individual and combined cognitive biases to generate adversarial prompts that can bypass LLM safety protocols while maintaining high attack success rates. The framework integrates supervised fine-tuning and reinforcement learning to optimize bias combinations for maximum effectiveness.

## Installation
```bash
git clone https://github.com/your-repo/CognitiveAttack.git
cd CognitiveAttack
pip install -r requirements.txt
```

## Quick Start
To train the CognitiveAttack red-team model:
```bash
python main.py
```

## Evaluation
To evaluate the trained model against target LLMs:

```bash
python attack.py
```

## Ethical Considerations
⚠️ Important: This tool is designed for research purposes and responsible AI safety evaluation. Please:

Use only for authorized red-teaming and safety research
Follow responsible disclosure practices
Do not use for malicious purposes
Refer to our ethics guidelines in the paper

## Citation
If you use CognitiveAttack in your research, please cite:
```
wait...
```

## License
This project is released under the MIT License for research use only.